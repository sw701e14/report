\section{Clustering}\label{clustering}
A clustering is a collection of data object groupings (clusters).
In order to identify hotspots we consider different clustering techniques.
This section will describe the concepts of clustering as well as the possible algorithms.
This section is based on \citet{pang2006introduction}.

\subsection{Cluster Analysis}
Cluster analysis is a technique used to group data objects into clusters based only on the information the data itself contains.
The requirement for membership in a certain cluster is often vague, as several clusterings can be made on the same dataset.
An example of this can be seen in \Cref{clusterings}, where the same dataset has been clustered in three radically different ways, even though all three could be regarded as correct, depending on the purpose of the clustering.
The clustering therefore depends on the dataset and its purpose.

\begin{figure}[H]
\includegraphics[trim= 3.1cm 19.91cm 5cm 2.5cm, clip=true]{graphics/different_clusters}
\centering
\caption{Different clusterings on the same dataset. From \citet{pang2006introduction}.}
\label{clusterings}
\end{figure}

\subsection{Types of clusterings}
This section will introduce the terminology used to describe the various types of clusterings. 

\paragraph{Hierarchical versus partitional}
A \textit{partitional clustering} is a division of data into pairwise disjoint subsets, where each object belongs to exactly one subset.
If clusters are allowed to have subclusters, the clustering is said to be \textit{hierarchical}.
A hierarchical clustering is represented as a set of nested clusters organized as a tree, where each node is the union of its children.

\paragraph{Exclusive, overlapping and fuzzy clusterings}

A clustering is \textit{exclusive} if an object is assigned to exactly one cluster.
If an object can belong to more than one group, the clustering is said to be \textit{overlapping}.
If a weight is used to describe the membership of sets, the clustering is said to be \textit{fuzzy}.

\paragraph{Complete versus partial}

A \textit{complete} clustering has every object assigned to a cluster, while a \textit{partial} clustering can have outliers; objects that do not belong to any cluster.

\subsection{Types of clusters}
The purpose of a cluster depends on the kind of data set it is applied on.
In this section, the different notions of a cluster will be presented.

\paragraph{Well separated}
A cluster is a set of objects, where the objects that are similar are grouped in a cluster. 
Sometimes a threshold is used to define a minimum similarity. 
All objects in a cluster need to be at least as similar to all other objects in the cluster, for an object to be in the cluster.

\paragraph{Prototype based}
Objects are placed in clusters based on the prototypes that define the clusters.
An object is placed in the cluster where the object is more similar to the prototype of that cluster, than to the prototype of all other clusters.

These prototypes can be either the average value of a cluster, or the most representative object of a cluster.

\paragraph{Graph based}
If the data can be represented as a graph, with the objects as nodes, clusters can be defined as connected components in the graph.

\paragraph{Density based}
A cluster is defined by the density of the data objects.
A cluster is a dense region surrounded by a low density region.

\subsection{Our data and purpose}\label{data_purporse}
The data we need to cluster is all the static GPS locations.
We expect the points to be distributed in a wide area, but clustered around places where people place the bikes (hotspots).
These clusters may have different size, depending on the physical properties of the places the bikes are left.

The purpose of the clustering is to find the areas where bikes are left most often, in order to make predictions on when a bike will arrive in those areas.
These areas can then be considered our hotspots.

Based on this description we need a partitional, exclusive, partial and density based clustering.

Partitional, because we do not need hierarchies for our problem, we only need the select clusters that meet our requirements for a cluster.
It needs to be exclusive because we want the clusters to be separated from each other so any point on the map is either in one cluster or not in any cluster.
We want to find out where the bikes spend most of their time and therefore we want a density based partial clustering.

\subsection{Techniques}
This section will explore the techniques that exist in cluster analysis, as well as evaluate them, based on the requirements stated in the previous section.

\subsubsection{K-means}
K-means is a technique that creates a partitioning from prototypes, where the prototypes are the centers of clusters.

The basic algorithm takes a number $K$ and generates $K$ initial center points.
Each data object is then assigned to the nearest center point.
The center points are now updated to be the center of the created clusters.
These steps are repeated until no point changes cluster, or the center points do not change.

Because we do not have a way of finding $K$ before running the algorithm, this approach is not applicable to our problem.

\subsubsection{Hierarchical Clustering}
Hierarchical clustering arranges the points in a hierarchy, dependent on the distance between points.
Hierarchical clustering can be performed either by starting at the top of the tree or at the bottom.
\textit{Agglomerative hierarchical clustering} starts by considering all points as being in individual clusters, then generating the hierarchy by merging the closest clusters.
\textit{Divisive hierarchical clustering} starts by considering all points as being in a single cluster, then continues by splitting the clusters at each step.

We would like our clustering to be partitional, which makes this algorithm inappropriate for hour purposes.
It should also be noted that hierarchical clustering is a complete clustering.
For bikes used by many different users one would expect some noise in the data.
Therefore it seems reasonable to only implement a partial clustering.

\subsubsection{DBSCAN}\label{clustering:DBSCAN}
Of the three presented algorithms, only the DBSCAN algorithm fits our need for an exclusive, partial and density-based clustering algorithm.
Thus we chose to implement this algorithm.
The algorithm locates regions of high density separated by regions of low density.
\alexander{Læs og vurder om intro til DBSCAN nu er acceptabel.}

\paragraph{Center based point density}
There exists different methods of determining density in a set of data points.
One approach is to use a point as a center and calculate the number of points that are within a certain radius of the point.
Using this measure it can be determined whether a point is placed in a dense region (a core point), at the edge of a dense region (a border point), or is in a region with sparse density (a noise point).
A visual representation can be seen on \Cref{dbscan_point_types} and the exact definition of the three types of points are as follows\cite{pang2006introduction}:

\begin{description}
\item[Core point] These points have at least $ MinPts $ points in a radius of $ eps $ from it.
Here, $ MinPts $ is the minimal number of points in the vicinity of a point, for it to be regarded as dense by the user, and $ eps $ is the radius in which to look for these points.

\item[Border points] A point that is not a core point, but is in the neighbourhood of a core point. 
One point can be a border point to several core points.

\item[Noise point] Any point that is neither a core point or a border point. 
\end{description}

\begin{figure}
\fbox{\includegraphics[trim= 3.1cm 20.61cm 5cm 2.5cm, clip=true]{graphics/DBSCAN_point_types}}
\caption{The three types of points in the clustering algorithm DBSCAN. Taken from \cite[page 528]{pang2006introduction}.}
\label{dbscan_point_types}
\end{figure}

Given these definitions, the algorithm is described in \Cref{dbscan-algo}\bruno{Ref til kilde. Kilden skal være slided for DBSCAN fra DIS.}.

\begin{algorithm}[h]
\SetAlgoNoEnd
DBSCAN(D, eps, MinPts)\\
	C $\leftarrow$ 0\\
	\ForEach {unvisited point P in dataset D}
	{
  	mark P as visited\\
		NeighborPts $\leftarrow$ regionQuery(D, P, eps)\\
		\eIf {sizeof(NeighborPts) < MinPts}{
			mark P as NOISE\\
		}
		{
			C $\leftarrow$ next cluster\\
			expandCluster(D, P, NeighborPts, C, eps, MinPts)
		}
	}
\caption{The DBSCAN clustering algorithm}\label{dbscan-algo}
\phantom{ForLineBreak}
\setcounter{AlgoLine}{0}

expandCluster(D, P, NeighborPts, C, eps, MinPts)\\
	add P to cluster C\\
	\ForEach {point Pn in NeighborPts}
	{
		\If{Pn is not visited}
		{
			mark Pn as visited\\
			NeighborPtsn $\leftarrow$ regionQuery(D, Pn, eps)\\
					\If {sizeof(NeighborPtsn) $\geq$ MinPts}
					{
						NeighborPts $\leftarrow$ $ NeighborPts \cup NeighborPtsn $
					}
		}
					\If {Pn is not yet member of any cluster}
					{
						{add Pn to cluster C\\}
					}
	}
%(Pn) is used instead of (p') in source (wiki).
\phantom{ForLineBreak}
\setcounter{AlgoLine}{0}

regionQuery(D, P, eps)\\
	\Return {all points from D within the eps-neighborhood of P (including P)\\}
\end{algorithm}

\alexander{Hvad skal der stå i summary - Alt er allerede nævn i afsnittet? -Slet?}
\subsection{Summary} Of the three presented algorithms, only the DBSCAN algorithm fits our need for a partitional, exclusive, partial and density-based clustering algorithm.
We therefore use this algorithm to detect clusters in our data, to be used as the hotspots in our model.
