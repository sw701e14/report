\section{Markov Chains}\label{markov}
When modeling transitions between different states in a complex system it is often beneficial to model the process using a Markov chain.
A Markov chain describes the rates at which a stochastic process (see \cref{markov:stochastic}) transitions between different states while maintaining the Markov property (see \cref{markov:property}).
Thus Markov chains are also known as Markov processes.

\subsection{Stochastic process}\label{markov:stochastic}
A stochastic process describes a nondeterministic process, in terms of the probabilities of it being in different \emph{states} over time.
Such processes allow for the modeling of systems that have no single correct processing order.
Instead these processes are mapped in terms of the probability of what \emph{the next step} is.
The result of this is that the output of a stochastic process cannot be predetermined, only the probabilities of different outputs.

Stochastic processes are classified in terms of the cardinality of their index and state space.
In this project \mikkel{We've had the discussion about 'this project' and the like before - what should we do for this semester?} we will focus on discrete time and discrete state space stochastic processes.
\mikkel{I'll wait with the rest of this section as I am not sure what we will do.
As I see it continuous time is better related to our problem, however i ''think'' that we could simulate the situation using discrete time.
My problem is that there are more/better sources on discrete time so that is what the theory below is based on.}

\subsection{Markov property}\label{markov:property}
A stochastic process is said to have the Markov property if for a given state in the process the next state can be determined without knowledge of prior states.
In general we can describe the transitions between states in a stochastic process as the following:

Let $X_n$ denote the state of a process after $n$ transitions and $P_{i,j}$ describe the probability that a process transitions from state $i$ to state $j$.
A stochastic process then has a finite set of states, given by $S = \{X_0, X_1, X_2,\cdots, X_{m-1}, X_m\}$.
Given an initial state $X_0$ (see \cref{markov:initialstate}) we have that:
\begin{align*}
P_{X_0,X_1} = \mathbf{P}&(X_1 \mid X_0)\\
P_{X_1,X_2} = \mathbf{P}&(X_2 \mid X_1, X_0)\\
\vdots\\
P_{X_{m-1},X_m} = \mathbf{P}&(X_m \mid X_{m-1}, \cdots, X_1, X_0)
\end{align*}
That is, we can describe the probability of each transition in a stochastic process in terms of its preceding transitions:
\begin{equation}\label{markov:eq:stochastic_prob}
P_{i,j} = \mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \cdots, X_0)
\end{equation}

The Markov property states that any given any state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

If for all $i \neq j \in S$ in a stochastic process it is true that:
\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \cdots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}
then the process has the Markov property and thus it is called a Markov process/Markov chain.

\subsection{The initial state}\label{markov:initialstate}