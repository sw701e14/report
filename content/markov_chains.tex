\section{Predicting behavior}
As the purpose of \projectname{} is to be able to predict the movement of bikes at a given time (see \mikkel{Insert reference to problem statement}), it must be able to answer questions such as the following:
\begin{itemize}
\item How long time will it take before a bike, with probability 90\%, will be in my vicinity?
\item What is the probability of find a bike nearby from 16.25 to 16.35?
\end{itemize}

The location of the bikes are abstracted by creating hotspots, as described in \cref{hotspots}.
In actuality, the location of bikes is modeled to be more complex as will be further explored in \cref{markov:modeling}.
However the theory and methods described in the following sections apply regardless.

By abstracting the locations to be hotspots the above questions can be rephrased as:

\begin{itemize}
\item How long time will it take before a bike, with probability 90\%, will be \textit{in a given hotspot}?
\item What is the probability of find a bike \textit{in a given hotspot} from 16.25 to 16.35?
\end{itemize}

To answer these questions we need to model the behavior of an \textit{average} bike.
In other words, we need to provide a formal description of the movement of a single bike representing the movement, on average, of all bikes.

\subsection{Abstraction Model}
As the system is based on anonymous usage no user sensitive information is collected.
Because of this uncertainty in the modeling is expected, as all bikes (and thus all users) are modeled as a single \textit{average bike}.
Modeling the movement of all bikes will be done by modeling the movement of this average bike.

Asides from that we must also consider that our model will be working on discrete-time location data.
The GPS units that the bikes will be equipped with will only report the location of a bike at a \textit{fixed} interval.
Additionally the accuracy of each of these measurements can be low, resulting in even more in-precision.

As we are modeling the average bike, we can represent the state of the process as the location of the bike.
The location of the bike can be seen as which hotspot the bike i currently located at.
Using this definition we can describe the movement of the bike as an infinite route $h_1, h_2, h_3, \dots$ where $h_i \in H \mid i \in \mathbb{N}$ and $H$ is the set of hotspots in the system.

Using this representation of the average bike we see that the average bike must move from one hotspot $h_i$ to another $h_{i+1}$ in one \textit{timestep}.
Note that it could be that $h_i = h_{i+1}$, meaning that the bike has moved to the hotspot it departed.
This is also used to indicate that the bike has not moved.
In \cref{markov:modeling} we will describe the modeling of the average city bike in greater detail.

Taking all of the above into account, it is reasonable to suggest the abstraction model, for the behavior of the average city bike, being a discrete-time Markov chain, where the states are used to represent hotspots.

\subsection{Discrete-Time Markov Chains}\label{markov}
A discrete-time Markov chain models state changes in a process at discrete time intervals.
It can be viewed as a sequence (chain) of random variables $X_1, X_2, X_3, \dots$ where each such variable is a function $X_i:S \rightarrow x$.
Here $S$ is a countable set of states representing the possible states (hotspots) in the process described by the Markov chain and $x$ is a \textit{random} state from $S$.
The subscript describes the time associated with each of the variables.
Thus $X_i$ represents the possible states of the process at time $i$.
This notation allows us to establish a timespan as each step refers to a fixed interval.

This randomness will be biased towards certain states given its current state (and possibly time) resulting in a set of probabilities for each state given a random variable.
Let $\mathbb{P}(X_i = x)$ be the probability of $X_i$ being in state $x$.
The above will then allow us to express the probability of moving to a state $y$ given the path we have traversed.
In other words, knowing what state we are in and how we got to it allows us to determine the probability of the next state being $y$:

$$\mathbb{P}(X_{n+i} = y \mid X_1 = x_1, X_2 = x_2, \dots X_n = x)$$

\paragraph{Markov property}\label{markov:property}
For the sequence to describe a Markov chain it must additionally have the Markov property.
The Markov property states that for any state $x$ we can determine the probability of the next state being $y$ without any knowledge of the path that led to $x$.
We can formalize this as:

\begin{align}\label{markov:eq:markov_prob}
\mathbb{P}(&X_{n+i} = y \mid X_1 = x_1, X_2 = x_2, \dots X_n = x)\nonumber\\
= \mathbb{P}(&X_{n+i} = y \mid X_n = x)
\end{align}

\paragraph{Time-homogeneous Markov chains}
In an effort to simplify the problem of predicting the behavior of the average bike we will be using time-homogeneous Markov chains.
\mikkel{Do we have a better argument as to why we choose this approach?}
A Time-homogeneous Markov chain can also be thought of as a time-independent chain.
In other words, predicting the next state of the chain can be done without knowledge of the time:
\begin{equation}
\mathbb{P}(X_{m+1} = y \mid X_m = x) = \mathbb{P}(X_{n+1} = y \mid X_n = x)
\end{equation}

\textit{This simplification will mean that we are unable to determine a difference in behavior at different times in the process.
Thus we cannot, using a single model, represent different behaviors of the average bike in the morning/afternoon or in different days of the week.
To allow for this distinction multiple models could be generated, representing different datasets.
The system would then simply select the model that corresponds to the current time-of-day/day-of-week.}

\paragraph{Mathematical representation}
From the above definitions we have that a Markov chain $\mathcal{M}$ can be described as a function $\tau_M:S\times S \rightarrow [0;1]$ such that $\tau_M(x, y) = \mathbb{P}(X_{n + 1} = y \mid X_n = x)$.
As the chain is independent of time we can represent it using a single matrix containing all its transitional probabilities.
This is a simple representation that allows us to predict multiple steps into the future by applying the same matrix to some initial state multiple times.

\paragraph{Visual representation}
Markov chains are typically represented using a DAG\footnote{Directed Acyclic Graph} with weight on its edges.
In such a graph nodes represent states and the weight of edges represent the probability of transitioning between two states.
This is a direct mapping of the matrix described above.

In \cref{markov:model:example1} is an example Markov chain with states $\{A, B, C\}$.
From the diagram we see that the process will transition from state $C$ to state $A$ with probability $0.9$ and to state $B$ with probability $0.1$.
There is zero probability of the process \textit{staying} in state $C$.
Any such edges can be removed from the graph to simplify it.
Thus all \emph{missing} edges should be considered edges with weight 0.
We can also see from the graph that the sum of the weights of all outgoing edges on each node is 1.
\mikkel{This paragraph is included intentionally as we will have some edges with weight 0.
This will be described in \cref{markov:modeling}.}

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
bend angle=15,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$A$};
  \node[main node] (2) [above right = 4cm and 3cm of 1] {$B$};
  \node[main node] (3) [below right = 4cm and 3cm of 2] {$C$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
        edge [bend left, draw=gray] node {\color{gray} 0.0} (3)
    (2) edge [bend left] node {0.3} (1)
        edge [loop above] node {0.4} (2)
        edge [bend left] node {0.3} (3)
    (3) edge [bend left] node {0.9} (1)
        edge [loop right, draw=gray] node {\color{gray} 0.0} (3)
        edge [bend left] node {0.1} (2);
\end{tikzpicture}
\caption{A state diagram describing a Markov chain with states $\{A, B, C\}$}
\label{markov:model:example1}
\mikkel{This diagram should be replaced by the one representing bikes in the next section.}
\end{figure}

\subsection{Modeling the average bike}\label{markov:modeling}
\mikkel{This section will describe a few problems in the simple state-set described in this section and how we have chosen to expand on it.}

\input{content/markov_model_simple}
\input{content/markov_model_complex}
