\section{Choosing prediction method}\label{markov}
We strive to create a system which can predict the movement of city bikes at a given time.
With this kind of system we should be able to answer the two questions:
\begin{itemize}
\item How likely is it that a city bike will be nearby within x minutes?
\item What is the probability to find a city bike nearby within a given time interval (e.g. from 16.25 to 16.35)?
\end{itemize}

The location of the city bikes are abstracted by creating hotspots, as described in \cref{hotspots}.
Thus predicting the movement of an average city bike can be done by answering the following question:

\begin{itemize}
\item What is the probability to find the average city bike at a given hotspot within a given time or interval?
\end{itemize}

To answer this question we need to model the behavior of this average city bike, i.e. a formal description of a single city bikes movement, on average, from one hotspot to another.

\subsection{Choosing abstraction model}
When modeling the behavior of the average city bike, uncertainty is expected as the behavior should represent the collective behavior of all city bikes.
Further as we do not store information about the users, we can not distinguish between the users and their personal routes.
Another thing to consider, when choosing abstraction model, is that the location data is provided through GPS units mounted on each city bike.
The GPS units provides data at a fixed time interval and can have a low accuracy at times.

Taking all that into account, it is reasonable to suggest the abstraction model, for the behavior of the average city bike, being a discrete-time Markov chain, where the states are used to represent hotspots.

\subsection{Discrete Markov Chains}
A discrete Markov chain models the state changes at discrete time intervals.
These intervals are indexed by an integer variable $ n $ and the corresponding states are denoted by $ X_n $.
The Markov chain has a finite set of states, given $X_i \in S$, which have the Markov property.

\paragraph{Markov property}\label{markov:property}
The Markov property states that any given state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}

Thus saying that the behavior of the Markov chain will be the same at the current position regardless of previous visits.


\paragraph{Visual representation}
Markov chains are typically represented using a DAG\footnote{Directed Acyclic Graph}.
In such a graph nodes represent states and the weight of edges represent the probability of transitioning between two states.
Because of the Markov property, processes can be represented using this very simple graphical representation.

In \cref{markov:model:example1} is an example Markov chain with states $\{A, B, C\}$.
From the diagram we see that the process will transition from state $C$ to state $A$ with probability $0.9$ and to state $B$ with probability $0.1$.
There is zero probability of the process \textit{staying} in state $C$.
Any such edges can be removed from the graph to simplify it.
Thus all \emph{missing} edges should be considered edges with weight 0.
We can also see from the graph that the sum of the weights of all outgoing edges on each node is 1.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
bend angle=15,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$A$};
  \node[main node] (2) [above right = 4cm and 3cm of 1] {$B$};
  \node[main node] (3) [below right = 4cm and 3cm of 2] {$C$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
        edge [bend left, draw=gray] node {\color{gray} 0.0} (3)
    (2) edge [bend left] node {0.3} (1)
        edge [loop above] node {0.4} (2)
        edge [bend left] node {0.3} (3)
    (3) edge [bend left] node {0.9} (1)
        edge [loop right, draw=gray] node {\color{gray} 0.0} (3)
        edge [bend left] node {0.1} (2);
\end{tikzpicture}
\caption{A state diagram describing a Markov chain with states $\{A, B, C\}$}
\label{markov:model:example1}
\end{figure}

\alexander{Conclusion of section?}