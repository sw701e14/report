\section{Markov Chains}\label{markov}
When modeling transitions between different states in a complex system it is often beneficial to model the process using a Markov chain.
A Markov chain describes the rates at which a stochastic process (see \cref{markov:stochastic}) transitions between different states while maintaining the Markov property (see \cref{markov:property}).
Thus Markov chains are also known as Markov processes.

\subsection{Stochastic process}\label{markov:stochastic}
A stochastic process describes a nondeterministic process, in terms of the probabilities of it being in different \emph{states} over time.
Such processes allow for the modeling of systems that have no single correct processing order.
Instead these processes are mapped in terms of the probability of what \emph{the next step} is.
The result of this is that the output of a stochastic process cannot be predetermined, only the probabilities of different outputs.

\paragraph{Classifications}
Stochastic processes are classified in terms of the cardinality of their index and state space.
Let $t$ refer to a timestamp and $X_t$ to the state of a stochastic process at time $t$, then if both $t$ and $X_t$ belong to $\mathbb{N}$ then the process can be modeled using Markov chains, given that the process has the Markov propery (\cref{markov:property}).

It is also possible to model a Markov chain using continuous time.
The advantage to this approach is the ability to be more exact in terms of time passed between state changes.
In a discrete time model, time can be considered as a collection of timestamps with a fixed interval.

\mikkel{Insert reference to section about GPS units, when it is written.}
The data received from the GPS units mounted on the bikes will be delivered to the system at some fixed interval.
Thus the data that the Markov chain is to model is quite sparse.
Using continuous time to model a process with such data will therefore not yield any additional information as opposed to a discrete time model.
As a continuous time model is more complex \mikkel{I am in need of citation} to model and will not yield better results, \projectname{} will use a discrete time model.

\subsection{Markov property}\label{markov:property}
A stochastic process is said to have the Markov property if for a given state in the process the next state can be determined without knowledge of prior states.
In general we can describe the transitions between states in a stochastic process as the following:

Let $X_n$ denote the state of a process after $n$ transitions and $P_{i,j}$ describe the probability that a process transitions from state $i$ to state $j$.
A stochastic process then has a finite set of states, given by $S = \{X_0, X_1, X_2,\dots, X_{m-1}, X_m\}$.
Given an initial state $X_0$ we have that:
\begin{align*}
P_{X_0,X_1} = \mathbf{P}&(X_1 \mid X_0)\\
P_{X_1,X_2} = \mathbf{P}&(X_2 \mid X_1, X_0)\\
\vdots\\
P_{X_{m-1},X_m} = \mathbf{P}&(X_m \mid X_{m-1}, \dots, X_1, X_0)
\end{align*}
That is, we can describe the probability of each transition in a stochastic process in terms of its preceding transitions:
\begin{equation}\label{markov:eq:stochastic_prob}
P_{i,j} = \mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0)
\end{equation}

The Markov property states that any given any state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

If for all $i \neq j \in S$ in a stochastic process it is true that:
\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}
then the process has the Markov property and thus it is called a Markov process/Markov chain.

\subsection{Representing a Markov chain}
In the sections above the property describing a Markov chain were presented as well as the overall type of processes that are modeled using Markov chains.
Below is given a formal definition of Markov chains, in terms of their mathematical and visual representations:

\paragraph{Mathematical representation}
A discrete-time Markov chain can be represented as a 2-tuple.
Below is given such a definition of Markov chains using such a representation:
\begin{align}
M & = (S, \tau)\\
S & = \{X_0, X_1, \dots, X_n\} \nonumber\\
\tau & \; : \; S \times S \rightarrow [0; 1] \nonumber
\end{align}
Where $S$ is a finite set of states in the process, as described in \cref{markov:property} and $\tau$ is a function that given two states $i$ and $j$ returns the probability of transitioning from state $i$ to $j$.
We denote this probability as $\tau_{ij}$ and note that it must hold that:
\begin{equation}\label{markov:eq:state_sum}
\forall s \in S \: \sum_{p \in S} (\tau_{sp}) = 1
\end{equation}
That is, there is probability 1 that the process will transition to its \emph{next state} when it transitions to its next state.

\paragraph{Visual representation}
Markov chains are typically represented using a directed acyclic graph.
In such a graph nodes represent states and the weight of edges represent the probability of transitioning between two states.
Because of the Markov property, processes can be represented using this very simple graphical representation.

In \cref{markov:model:example1} is an example Markov chain with states $\{A, B, C\}$.
From the diagram we see that the process will transition from state $C$ to state $A$ with probability $0.9$ and to state $B$ with probability $0.1$.
There is zero probability of the process \textit{staying} in state $C$.
Any such edges can be removed from the graph to simplify it.
Thus all \emph{missing} edges should be considered edges with weight 0.
We can also see from the graph that the sum of the weights of all outgoing edges on each node is 1, as per \cref{markov:eq:state_sum}.

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
bend angle=15,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$A$};
  \node[main node] (2) [above right = 4cm and 3cm of 1] {$B$};
  \node[main node] (3) [below right = 4cm and 3cm of 2] {$C$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
        edge [bend left, draw=gray] node {\color{gray} 0.0} (3)
    (2) edge [bend left] node {0.3} (1)
        edge [loop above] node {0.4} (2)
        edge [bend left] node {0.3} (3)
    (3) edge [bend left] node {0.9} (1)
        edge [loop right, draw=gray] node {\color{gray} 0.0} (3)
        edge [bend left] node {0.1} (2);
\end{tikzpicture}
\caption{A state diagram describing a Markov chain with states $\{A, B, C\}$}
\label{markov:model:example1}
\end{figure}
