\section{Predicting behavior}
As the purpose of \projectname{} is to be able to predict the movement of bikes at a given time (see \mikkel{Insert reference to problem statement}), it must be able to answer questions such as the following:
\begin{itemize}
\item How long time will it take before a bike, with probability 90\%, will be in my vicinity?
\item What is the probability of find a bike nearby from 16.25 to 16.35?
\end{itemize}

The location of the bikes are abstracted by creating hotspots, as described in \cref{hotspots}.
In actuality, the location of bikes is modeled to be more complex as will be further explored in \cref{markov:modeling}.
However the theory and methods described in the following sections apply regardless.

By abstracting the locations to be hotspots the above questions can be rephrased as:

\begin{itemize}
\item How long time will it take before a bike, with probability 90\%, will be \textit{in a given hotspot}?
\item What is the probability of find a bike \textit{in a given hotspot} from 16.25 to 16.35?
\end{itemize}

To answer these questions we need to model the behavior of an \textit{average} bike.
In other words, we need to provide a formal description of the movement of a single bike representing the movement, on average, of all bikes.

\subsection{Abstraction model}
When modeling the behavior of the average city bike, uncertainty is expected as the behavior should represent the collective behavior of all city bikes.
Further as we do not store information about the users, we can not distinguish between the users and their personal routes.
Another thing to consider, when choosing abstraction model, is that the location data is provided through GPS units mounted on each city bike.
The GPS units provides data at a fixed time interval and can have a low accuracy at times.

Taking all that into account, it is reasonable to suggest the abstraction model, for the behavior of the average city bike, being a discrete-time Markov chain, where the states are used to represent hotspots.

\subsection{Discrete Markov Chains}\label{markov}
A discrete Markov chain models the state changes at discrete time intervals.
These intervals are indexed by an integer variable $ n $ and the corresponding states are denoted by $ X_n $.
The Markov chain has a finite set of states, given $X_i \in S$, which have the Markov property.

\paragraph{Markov property}\label{markov:property}
The Markov property states that any given state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}

Thus saying that the behavior of the Markov chain will be the same at the current position regardless of previous visits.


\paragraph{Visual representation}
Markov chains are typically represented using a DAG\footnote{Directed Acyclic Graph}.
In such a graph nodes represent states and the weight of edges represent the probability of transitioning between two states.
Because of the Markov property, processes can be represented using this very simple graphical representation.

In \cref{markov:model:example1} is an example Markov chain with states $\{A, B, C\}$.
From the diagram we see that the process will transition from state $C$ to state $A$ with probability $0.9$ and to state $B$ with probability $0.1$.
There is zero probability of the process \textit{staying} in state $C$.
Any such edges can be removed from the graph to simplify it.
Thus all \emph{missing} edges should be considered edges with weight 0.
We can also see from the graph that the sum of the weights of all outgoing edges on each node is 1.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
bend angle=15,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$A$};
  \node[main node] (2) [above right = 4cm and 3cm of 1] {$B$};
  \node[main node] (3) [below right = 4cm and 3cm of 2] {$C$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
        edge [bend left, draw=gray] node {\color{gray} 0.0} (3)
    (2) edge [bend left] node {0.3} (1)
        edge [loop above] node {0.4} (2)
        edge [bend left] node {0.3} (3)
    (3) edge [bend left] node {0.9} (1)
        edge [loop right, draw=gray] node {\color{gray} 0.0} (3)
        edge [bend left] node {0.1} (2);
\end{tikzpicture}
\caption{A state diagram describing a Markov chain with states $\{A, B, C\}$}
\label{markov:model:example1}
\end{figure}

\subsection{Modeling the average bike}\label{markov:modeling}

\alexander{Conclusion of section?}