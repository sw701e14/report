\section{Markov Chains}\label{markov}
When modeling transitions between different states in a complex system it is often beneficial to model the process using a Markov chain.
A Markov chain describes the rates at which a stochastic process (see \cref{markov:stochastic}) transitions between different states while maintaining the Markov property (see \cref{markov:property}).
Thus Markov chains are also known as Markov processes.

\subsection{Stochastic process}\label{markov:stochastic}
A stochastic process describes a nondeterministic process, in terms of the probabilities of it being in different \emph{states} over time.
Such processes allow for the modeling of systems that have no single correct processing order.
Instead these processes are mapped in terms of the probability of what \emph{the next step} is.
The result of this is that the output of a stochastic process cannot be predetermined, only the probabilities of different outputs.

\paragraph{Classifications}
Stochastic processes are classified in terms of the cardinality of their index and state space.
Let $t$ refer to a timestamp and $X_t$ to the state of a stochastic process at time $t$, then if both $t$ and $X_t$ belong to $\mathbb{N}$ then the process can be modeled using Markov chains, given that the process has the Markov propery (\cref{markov:property}).

It is also possible to model a Markov chain using continuous time.
The advantage to this approach is the ability to be more exact in terms of time passed between state changes.
In a discrete time model, time can be considered as a collection of timestamps with a fixed interval.

\mikkel{Insert reference to section about GPS units, when it is written.}
The data received from the GPS units mounted on the bikes will be delivered to the system at some fixed interval.
Thus the data that the Markov chain is to model is quite sparse.
Using continuous time to model a process with such data will therefore not yield any additional information as opposed to a discrete time model.
As a continuous time model is more complex \mikkel{I am in need of citation} to model and will not yield better results, \projectname{} will use a discrete time model.

\subsection{Markov property}\label{markov:property}
A stochastic process is said to have the Markov property if for a given state in the process the next state can be determined without knowledge of prior states.
In general we can describe the transitions between states in a stochastic process as the following:

Let $X_n$ denote the state of a process after $n$ transitions and $P_{i,j}$ describe the probability that a process transitions from state $i$ to state $j$.
A stochastic process then has a finite set of states, given by $S = \{X_0, X_1, X_2,\cdots, X_{m-1}, X_m\}$.
Given an initial state $X_0$ (see \cref{markov:initialstate}) we have that:
\begin{align*}
P_{X_0,X_1} = \mathbf{P}&(X_1 \mid X_0)\\
P_{X_1,X_2} = \mathbf{P}&(X_2 \mid X_1, X_0)\\
\vdots\\
P_{X_{m-1},X_m} = \mathbf{P}&(X_m \mid X_{m-1}, \cdots, X_1, X_0)
\end{align*}
That is, we can describe the probability of each transition in a stochastic process in terms of its preceding transitions:
\begin{equation}\label{markov:eq:stochastic_prob}
P_{i,j} = \mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \cdots, X_0)
\end{equation}

The Markov property states that any given any state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

If for all $i \neq j \in S$ in a stochastic process it is true that:
\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \cdots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}
then the process has the Markov property and thus it is called a Markov process/Markov chain.

\subsection{The initial state}\label{markov:initialstate}

\begin{figure}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=10cm,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$B_1$};
  \node[main node] (2) [right of=1] {$B_2$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
    (2) edge [bend left] node {0.6} (1)
        edge [loop right] node {0.4} (2);
\end{tikzpicture}
\caption{A Markov chain model of two bike stations}
\label{markov:model:simple}
\end{figure}

\begin{figure}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (B1) {$B_1$};
  \node[main node,font=\sffamily\small] (D1) [right = 2cm of B1] {$D_1$};
  \node[main node,font=\sffamily\small] (D2) [right of=D1] {$D_2$};
  \node[main node] (B2) [right = 2cm of D2] {$B_2$};

  \path[every node/.style={font=\sffamily\small}]
    (B1) edge [bend left] node {0.05} (B2)
         edge [loop left] node {0.65} (B1)
         edge [bend left] node[below] {0.3} (D1)
    (B2) edge [bend left] node {0.15} (B1)
         edge [loop right] node {0.35} (B2)
         edge [bend left] node[above] {0.5} (D2)
    (D1) edge [bend left] node[above] {0.1} (B1)
         edge [loop right] node {0.6} (D1)
         edge [bend left] node[below left] {0.3} (B2)
    (D2) edge [bend left] node[below] {0.1} (B2)
         edge [loop left] node {0.7} (D2)
         edge [bend left] node[above right] {0.2} (B1);
\end{tikzpicture}
\caption{A Markov chain model of two bike stations with departure states}
\label{markov:model:complex}
\end{figure}
