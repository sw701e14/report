\section{Choosing prediction method}\label{markov}
We strive to create a system which can predict the movement of city bikes at a given time. With this kind of system we should be able to answer the two questions:
\begin{itemize}
\item How likely is it that a city bike will be nearby within x minutes?
\item What is the probability to find a city bike nearby within a given time interval (e.g. from 16.25 to 16.35)?
\end{itemize}

The location of the city bikes are abstracted by creating hotspots, as described in \cref{hotspots}. Thus predicting the movement of an average city bike can be done by answering the following question:

\begin{itemize}
\item What is the probability to find the average city bike at a given hotspot within a given time or interval?
\end{itemize}

To answer this question we need to model the behavior of this average city bike, i.e. a formal description of a single city bikes movement, on average, from one hotspot to another.

\subsection{Choosing abstraction model}
When modeling the behavior of the average city bike, uncertainty is expected as the behavior should represent the collective behavior of all city bikes.
Further as we do not store information about the users, we can not distinguish between the users and their personal routes.
Another thing to consider, when choosing abstraction model, is that the location data is provided through GPS units mounted on each city bike. The GPS units provides data at a fixed time interval and can have a low accuracy at times.

Taking all that into account, it is reasonable to suggest the abstraction model, for the behavior of the average city bike, being a discrete-time Markov chain, where the states are used to represent hotspots. 




\subsection{Markov Chains}












































\section{Markov Chains}\label{markov}
The system needs a way to make predictions of where a bike is likely to be in the future.
The users of the bikes will use this feature to look for a bike that will arrive in their vicinity in the near future.

In order to do this we need a way to model the behaviour of the bikes, i.e. how the bikes move from one hotspot to another.
This behaviour contains a degree of uncertainty, and a stochastic model is therefore appropriate \cite[p. ~296]{bertsekas2002introduction}.

\mikkel{Insert reference to section about GPS units, when it is written.}
The data received from the GPS units mounted on the bikes will be delivered to the system at some fixed interval.
This suggest that a discrete markov chain model may be used.
The following will provide a short description on markov chains based on \citet[chapter 7]{bertsekas2002introduction}

\subsection{Discrete markov chains}\label{markov:intro}
In a discrete markov chain model the state changes at discrete time intervals. 
These intervals are indexed by an integer variable $ n $ and the corresponding states are denoted by $ X_n $.
The markov chain has a finite set of states, given $X_i \in S$.
%by $S = \{X_0, X_1, X_2,\dots, X_{m-1}, X_m\}$\alexander{\giovanni{No it is $X_i \epsilon S$}} also known as the state space.

A markov chain can be described by its transitional probabilities \begin{align}
P_{i,j} = \mathbf{P} (X_{n+1} = j\mid X_n = i)
\end{align}

This states that whenever the state is $ i $, there is a probability of $ P_{i,j}  $ that the next state will be $ j $

\paragraph{Markov property}\label{markov:property}
A stochastic process is said to have the Markov property if for a given state in the process the next state can be determined without knowledge of prior states.
In general we can describe the transitions between states in a stochastic process as the following:

Given an initial state $X_0$ we have that:
\begin{align*}
P_{X_0,X_1} = \mathbf{P}&(X_1 \mid X_0)\\
P_{X_1,X_2} = \mathbf{P}&(X_2 \mid X_1, X_0)\\
\vdots\\
P_{X_{m-1},X_m} = \mathbf{P}&(X_m \mid X_{m-1}, \dots, X_1, X_0)
\end{align*}
That is, we can describe the probability of each transition in a stochastic process in terms of its preceding transitions:
\begin{equation}\label{markov:eq:stochastic_prob}
P_{i,j} = \mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0)
\end{equation}

The Markov property states that any given any state $i \in S$ in a process we can determine the next state $j$ without knowledge of any states prior to $i$.
This allows us to simplify \cref{markov:eq:markov_prob} for Markov chains and provide a definition of the Markov property:

If for all $i \neq j \in S$ in a stochastic process it is true that:
\begin{align}\label{markov:eq:markov_prob}
P_{i,j} = &\mathbf{P}(X_{n+1} = j \mid X_n = i, X_{n-1}, \dots, X_0) \nonumber\\
        = &\mathbf{P}(X_{n+1} = j \mid X_n = i)
\end{align}
then the process has the Markov property and thus it is called a Markov process/Markov chain.

\subsection{Representing a Markov chain}
In the sections above the property describing a Markov chain were presented as well as the overall type of processes that are modeled using Markov chains.
Below is given a formal definition of Markov chains, in terms of their mathematical and visual representations:

\paragraph{Mathematical representation}
A discrete-time Markov chain can be represented as a pair.
%2-tuple\alexander{\giovanni{2-tuple == pair?}}.
Below is given such a definition of Markov chains using such a representation:
\begin{align}
M & = (S, \tau)\\
S & = \{X_0, X_1, \dots, X_n\} \nonumber\\
\tau & \; : \; S \times S \rightarrow [0; 1] \nonumber
\end{align}
Where $S$ is a finite set of states in the process, as described in \cref{markov:intro} and $\tau$ is a function that given two states $i$ and $j$ returns the probability of transitioning from state $i$ to $j$.
We denote this probability as $\tau_{ij}$ and note that it must hold that:
\begin{equation}\label{markov:eq:state_sum}
\forall s \in S \: \sum_{p \in S} (\tau_{sp}) = 1
\end{equation}
That is, there is probability 1 that the process will transition to its \emph{next state} when it transitions to its next state.

\paragraph{Visual representation}
Markov chains are typically represented using a DAG\footnote{Directed Acyclic Graph}.
In such a graph nodes represent states and the weight of edges represent the probability of transitioning between two states.
Because of the Markov property, processes can be represented using this very simple graphical representation.

In \cref{markov:model:example1} is an example Markov chain with states $\{A, B, C\}$.
From the diagram we see that the process will transition from state $C$ to state $A$ with probability $0.9$ and to state $B$ with probability $0.1$.
There is zero probability of the process \textit{staying} in state $C$.
Any such edges can be removed from the graph to simplify it.
Thus all \emph{missing} edges should be considered edges with weight 0.
We can also see from the graph that the sum of the weights of all outgoing edges on each node is 1, as per \cref{markov:eq:state_sum}.

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
bend angle=15,
  thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

  \node[main node] (1) {$A$};
  \node[main node] (2) [above right = 4cm and 3cm of 1] {$B$};
  \node[main node] (3) [below right = 4cm and 3cm of 2] {$C$};

  \path[every node/.style={font=\sffamily\small}]
    (1) edge [bend left] node {0.3} (2)
        edge [loop left] node {0.7} (1)
        edge [bend left, draw=gray] node {\color{gray} 0.0} (3)
    (2) edge [bend left] node {0.3} (1)
        edge [loop above] node {0.4} (2)
        edge [bend left] node {0.3} (3)
    (3) edge [bend left] node {0.9} (1)
        edge [loop right, draw=gray] node {\color{gray} 0.0} (3)
        edge [bend left] node {0.1} (2);
\end{tikzpicture}
\caption{A state diagram describing a Markov chain with states $\{A, B, C\}$}
\label{markov:model:example1}
\end{figure}

\subsection{Determining probabilities}

As a Markov chain can be represented using a DAG, it can also be thought of as an adjacency matrix $P$.
And similarly we can represent a set of probabilities of being in each state using a row-vector $r$.
To identify such sets at different timestamps, we let $r_t$ define the row vector describing the probability of being in each state at time $t$, given that we started with an initial vector $r_0$.

\begin{equation}
P = \begin{bmatrix}
    \tau_{{s_0}{s_0}} & \tau_{{s_0}{s_1}} & \tau_{{s_0}{s_2}} & \cdots & \tau_{{s_0}{s_n}} \\[0.3em]
    \tau_{{s_1}{s_0}} & \tau_{{s_1}{s_1}} & \tau_{{s_1}{s_2}} & \cdots & \tau_{{s_1}{s_n}} \\[0.3em]
    \tau_{{s_2}{s_0}} & \tau_{{s_2}{s_1}} & \tau_{{s_2}{s_2}} & \cdots & \tau_{{s_2}{s_n}} \\[0.3em]
          \cdots      &      \cdots       &      \cdots       & \ddots &     \vdots \\[0.3em]
   \tau_{{s_n}{s_0}} & \tau_{{s_n}{s_1}} & \tau_{{s_n}{s_2}} & \cdots & \tau_{{s_n}{s_n}}
     \end{bmatrix}
\end{equation}

With the example given in \cref{markov:model:example1} we can define a matrix describing the transitions and an initial set of probabilities.
As an example, we could start the process in state $A$.
That is, there is probability 1 of being in state $A$ at time 0:

\begin{equation}\label{markov:model:example1:initial}
r_0 = \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}
\quad\quad
P = \begin{bmatrix}
       0.7 & 0.3 & 0.0 \\[0.3em]
       0.3 & 0.4 & 0.3 \\[0.3em]
       0.9 & 0.1 & 0.0
     \end{bmatrix}
\end{equation}

To determine the probability of the process being in the different states is a matter of calculating the product of the row-vector and probability matrix:
$$r_n = r_{n-1} \cdot P$$
As a row-vector is defined recursively in terms of its predecessor, as per the Markov property (see \cref{markov:property}), we can define it in terms of its k\superscript{th} predecessor.
Using that we can describe any row-vector $r_n$ in terms of the initial row-vector $r_0$.
\begin{align}
r_n &= r_{n-k} \cdot P^k\\
    &= r_{n - n} \cdot P^n\nonumber\\
    &= r_0 \cdot P^n\nonumber
\end{align}

\alexander{\giovanni{What does $r_n$ represent?}}
